\subsection{Essential Supremum of a family of Random Variables}

\np For measure space $(\Omega, \Fs, \P)$, $\Phi$ an arbitrary famaly of random variables.\\
First case $\Phi$ countable,
    \begin{center}
        $\varphi^*(\omega) = \sup_{\varphi \in \Phi} \varphi(\omega)$ will also be a random variable.
    \end{center}



\begin{example}\ \\
$\Phi = \{\1_{x}:\ 0 < x \leq 1\}$, and $\Omega = [0,1],\ \P = \text{ Lebesgue measure}$, then $\sup_{\varphi \in \Phi} \varphi(\omega) = 1$.
\end{example}

\begin{theorem}\ \\
Let $\Phi$ be an arbitrary collection of eandom variables on $(\Omega, \Fs, \P)$,
\begin{enumerate}[label = (\alph*)]
    \item $\exists$ a random variable $\vphi^*$ s.t. $\vphi^* \geq \vphi$ $\P$-a.s. for all $\vphi \in \Phi$. Moreover, $\vphi^*$ is $\P$-a.s. unique:
    \begin{center}
         any other random variable $\vphi$ satisfies $\vphi \geq \vphi^*$ $\P$-a.s.
    \end{center}
    \item Suppose $\Phi$ is directed upwards (i.e. $\vphi \& \Tilde{\vphi} \in \Phi$, $\exists\ \psi \in \Phi$ s.t. $\psi \geq \vphi \vee \title{\vphi}$), which implies
    \begin{center}
        $\exists$ an increasing sequence $(\vphi_n)$ of r.v's in $\Phi$ s.t. $\vphi^* = \lim_{n} \vphi_n$ $\P$-a.s.
    \end{center}
\end{enumerate}
\end{theorem}
\begin{proof}\ 
\begin{enumerate}[label = (\arabic*)]
    \item WLOG, $\vphi \in \Phi$ takes values in $[0,1]$, otherwise 
\begin{equation*}
    \Tilde{\Phi} = \{f \circ \vphi:\ \vphi \in \Phi\}
\end{equation*}
    with $f : \R \to [0,1]$ is strictly increasing.
    \item If $\Psi (\subset \Phi)$ is countable, then $\vphi_{\Psi} (\omega) = \sup_{\vphi \in \Psi} \vphi(\omega)$ is measurable.\\
We claim that $c:= \sup \{\E[\vphi_{\Psi}]\ |\ \Psi \subset \Phi \text{ countable } \}$ is attained by countable $\Psi^* \subseteq \Phi$.\\
Let $(\Psi_n)$ be a maximizing sequence (i.e. $\E[\vphi_{\Psi_n}] \uparrow c$), we have
\begin{equation*}
    \Psi^* = \cup_n \Psi_n.
\end{equation*}
is also countable, and by MCT
\begin{equation*}
    \E[\underbrace{\vphi_{\Psi^*}}_{\vphi^*}] = c.
\end{equation*}
    \item We have $\vphi^* \geq \vphi \text{ a.s.},\ \vphi \in \Phi$. If not, $\exists$ a r.v. $\vphi \in \Phi$ s.t.
    \begin{equation*}
        \P(\vphi \geq \vphi^*) > 0.
    \end{equation*}
    Define $\Psi' = \Psi^* \cup \{\vphi\}$, we have
    \begin{equation*}
        \E[\vphi_{\Psi'}] > c,
    \end{equation*}
    which gives a contradiction.
    \item The uniqueness is easy.
\end{enumerate}
\end{proof}

\np Simple observation:\\
$H_n \downarrow H \Rightarrow \P(H_n) \downarrow \P(H)$, and if $H_1 \supset H_2 \supset \dots$, we have $\lim_n H_n = \cap_n H_n = H$. Then 
\begin{equation*}
    H_n^c \uparrow H^c \Rightarrow \P(H_n^c) \to \P(H^c),\ 1 - \P(H_n) \uparrow 1 - \P(H) 
\end{equation*}

\np We say $\mu$ is a distribution of $X$ by defining $\mu(A) = \P(X^{-1} A)$, and we have c.d.f $c(x) = \mu(-\infty,x]$, which is right continuous, increasing and $c(\infty) = 1$.

\vspace{12pt}
\subsection{Properties of Random Variables}
\begin{definition}[Independence]\ \\
For a probability space $(\Omega, \Hs,\P)$, we say $\Fs \indep \Gs$ provided $\P(A \cap B) = \P(A) \P(B),\ A \in \Fs,\ B \in \Gs$, where $\Fs,\Gs$ are two subalgebras of $\Hs$.
\end{definition}

\begin{remark}\ \\
Suppose $X,Y$ are two r.v.s $(\sigma(X), \sigma(Y))$, we have $ X \indep Y$ provided $\sigma(X)$ and $\sigma(Y)$ are independent. $X \indep \Gs$ if $\sigma(X)\ \&\ \Gs$ are independent.\\
Equivalently, $X \indep Y$ if $\pi(A \times B) = \P(X \in A,\ Y \in B) = \P(X \in A) \P(Y \in B)$.
\end{remark}

\np Recall that $\E[f_0 X] = \mu f$,
\begin{example}\ \\
$\mu(dx) = \frac{c^a x^{a - 1} e^{-cx}}{\Gamma(a)} \dr x,\ x \in \R^+$, where $a$ is the index parameter, $c$ is the scale parameter, and $\Gamma$ be the Gamma function. We have
\begin{equation*}
    \Gamma(a) = \int_0^\infty x^{a-1} e^{-x} \dr x,\ \Gamma(n) = (n-1)!,\ \Gamma(\frac{1}{2}) = \sqrt{2\pi}.
\end{equation*}
Let $X\ \&\ Y$ have Gamma distribution and $X \indep Y$, i.e. $X \sim \gamma_a,\ Y \sim \gamma_b$ with scale parameter $1$, we have
\begin{enumerate}[label = (\alph*)]
    \item $X + Y \sim \gamma_{a + b}$.
    \item $\frac{X}{X+Y}$ has distribution $\beta_{a,b} =\dr u\ \frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)}u^{a - 1} (1 - u)^{b-1}$.
    \item $X + Y \indep \frac{X}{X+Y}$.
    \begin{proof}
    The joint distribution $\pi$ on $(X+Y, \frac{X}{X+Y})$ is defined
    \begin{align*}
        \pi f &= \E f(X+Y, \frac{X}{X+Y})\\ 
        &= \int_0^\infty \dr x\ \frac{x^{a - 1} e^{-x}}{\Gamma(a)} \int_0^\infty \dr y\ \frac{e^{-y} x^{b - 1}}{\Gamma(b)} f(x + y, \frac{x}{x + y})\\
        &= \int_0^\infty \dr z\ \int_0^1 \dr u\ \frac{z^{a+b-1}e^{-z}}{\Gamma(a) \Gamma(b)} u^{a-z} (1 - u)^{b-1} f(z,u)  &(x \to uz,\ y = (1 - u)z)\\
        &= (\gamma_{a+b} \times \beta_{a+b})f
    \end{align*}
    \end{proof}
\end{enumerate}
\end{example}

\begin{example}\ \\
Suppose $X \sim \Ns(0,1)$, then we have the distribution $\nu$ of $X$ is given by
\begin{equation*}
    \nu(\dr x) = \dr x\ \frac{e^{x^2/2}}{\sqrt{2\pi}}.
\end{equation*}
Let $\mu$ be the distribution of $X^2$, then
\begin{equation*}
    \mu f = \E f \circ X^2 = \E g \circ X = \nu g,\ g(x) = f(x^2),
\end{equation*},
we further have 
\begin{equation*}
    \mu f = \int \dr x\ e^{-x^2/2} g(x) = \int \dr x\ e^{-x^2/2} f(x^2) = \int\ \mu( \dr y) f(y),
\end{equation*}
and thus
\begin{equation*}
    \mu(\dr y) = \dr y\ \frac{1}{\sqrt{2\pi}} \frac{e^{-y}}{\sqrt{y}} = \frac{\dr y\ e^{-y} y^{-1/2}}{\Gamma(1/2)}
\end{equation*}
\end{example}

\begin{example}\ \\
For $X \geq 0$, $r \to \E e^{-r X}$ decreasing continuous. In fact $\exists\ R$ r.v. s.t. 
\begin{equation*}
    \underbrace{\P(R > r)}_{\text{Survival Function}} = \E e^{-r X}.
\end{equation*}
Take $R = \frac{Y}{X},\ Y \indep X,\ Y \sim \exp$, which means $\P(Y > x) = e^{-x}$. We have
\begin{align*}
    \P(R > r) &= \P(Y > rX)\\
    &= \int_{\R_+} \mu(\dr x) \int_{\R_+}\dr y\ e^{-y} \1_{(rx,+\infty)}(y)\\
    &= \int \mu(\dr x) e^{-rx}\\
    &= \E e^{-r X}.
\end{align*}
\end{example}

\begin{example}\ \\
For $X \sim \gamma_{a,c}$, $\P(R > r) = \E e^{-rx} = (\frac{c}{c + r})^a$. ($R = \frac{X}{Y}$)
\end{example}

 
\begin{definition}\ 
\begin{itemize}
    \item $\E X^n$ is called the \textit{$n$th moment} of $X$.
    \item $\E[(X - \E X)^2]$ is called the \textit{variance} of $X$.
    \item $\hat{\mu}_r = \E e^{-rX}$ the \textit{Laplace transform} of $X$, useful for $X \geq 0$.
    \item $\widetilde{\mu}_r = \E e^{irX}$ the \textit{Fourier transform} of $X$ (also \textit{characteristic function}).
    \item $\E z^X = \sum_{n = 0}^\infty z^n \P(X = n)$ is called the \textit{moment generating function}.
    \item For $X \geq 0$, $\E X^p = \int_0^\infty \dr x\ p x^{p-1} \P(X > x)$, where
    \begin{equation*}
        \E X^p(\omega) = \int_0^{X(\omega)} \dr\ x\ p x^{p-1} \E[\1_{X > x}]
    \end{equation*}
    by Fubini's theorem.
\end{itemize}
\end{definition}

\begin{proposition}[Markov's Inequality]\ \\
For $X \geq 0,\ b > 0$, we have
\begin{equation*}
    \P(X > b) \leq \frac{\E X}{b}.
\end{equation*}
\end{proposition}
\begin{proof}\ \\
We have $X \geq b \1_{X > b}$, and integrate on both side, we get
\begin{equation*}
    \E X \geq b \E \1_{X > b},
\end{equation*}
which gives the inequality for free.
\end{proof}

\begin{proposition}[Chebyshev Inequality]\ \\
Suppose $\E X = a < \infty$, we have
\begin{equation*}
    \P[(X - a)^2 > \vep^2] \leq \frac{1}{\vep^2} \E[(X-a)^2],
\end{equation*}
or equivalently,
\begin{equation*}
    \P[\abs{X - a} > \vep] \leq \frac{\Var{X}}{\vep^2}
\end{equation*}
\end{proposition}


\vspace{6pt}
\begin{theorem}[Weierstrass Approximation]\ \\
If $f$ is continuous on $[0,1]$ and $\vep > 0$ then $\exists$ a polynomial $B$ s.t.
\begin{equation*}
    \sup_{x \in [0,1]} \abs{B(x) - f(x)} \leq \vep.
\end{equation*}
\end{theorem}
\begin{proof}
We have $\P(S_n = k) = \binom{n}{k}p (1 - p)^{n - k}$ the Binomial distribution. Define
\begin{equation*}
    B_n(p) = \E[f(\frac{S_n}{n})] = \sum_{k = 0}^n f(\frac{k}{n}) \underbrace{\binom{n}{k} p^k (1 - p)^{n - k}}_{\text{Bernstein Polynomials}}.
\end{equation*}
Note that $\exists\ K > 0$ s.t. $\abs{f(y)} \leq K$ and f is uniformly continuous ($\abs{x - y} \leq \delta \Rightarrow \abs{f(x) - f(y)} \leq \frac{1}{2} \ep$). We have
\begin{align*}
    \abs{B_n(p) - f(p)} = \abs{\E[f(\frac{S_n}{n}) - f(p)]}.
\end{align*}
Define $Y_n = \abs{f(\frac{S_n}{n}) - f(p)},\ Z_n = \abs{\frac{S_n}{n} - p}$, and
\begin{equation*}
    Z_n \leq \delta \Rightarrow Y_n < \frac{1}{2}.
\end{equation*}
Therefore,
\begin{align*}
    \abs{B_n(p) - f(p)} &\leq \E Y_n\\
    &= \E[Y_n \1_{Z_n \leq \delta}] + \E[Y_n \1_{Z_n > \delta}]\\
    &= \frac{1}{2} \ep + K \P(Z_n > \delta).
\end{align*}
Note by Chebyshev Inequality have
\begin{equation*}
    \P(Z_n > \delta) \leq \frac{\Var{\frac{S_n}{n}}}{\delta^2} = \frac{\frac{1}{n^2} \Var{S_n}}{\delta^2} = \frac{p(1 - p)}{n \delta^2} \leq \frac{1}{4n\delta^2}.
\end{equation*}
Thus we have $ \sup_{x \in [0,1]} \abs{B(x) - f(x)} \to 0$ as $n \to \infty$.
\end{proof}

\vspace{6pt}
\subsection{Convergence Concepts}

\subsubsection{Convergence Almost Surely}
\begin{definition}\ \\
$X_n \overset{\text{a.s.}}{\longrightarrow} X$ if $X_n(\omega) \to X(\omega)$ for a.e. $\omega \in \Omega$, which is also equivalent to
\begin{equation*}
    N_\vep = \sum_n \1_{(\vep, \infty)} (\abs{X_n - X}) < \infty,\ \forall\ \vep > 0\ \text{a.s.}.
\end{equation*}
\end{definition}

\vspace{6pt}
\begin{lemma}[Borel-Cantelli Lemma]\label{BCL}\ 
\begin{enumerate}[label = (\arabic*)]
    \item $(B_i)$ is a Bernoulli sequence $B_i = 0 \text{ or } 1$, we have
    \begin{equation*}
        \sum_n \P(B_i = 1) < \infty\ \Rightarrow\ \sum_n B_i < \infty \text{ a.s.}.
    \end{equation*}
    \item $(H_i)$ be events, then
    \begin{equation*}
        \sum_n \P(H_i) < \infty\ \Rightarrow\ \sum_n \1_{H_n} < \infty \text{ a.s.}.
    \end{equation*}
\end{enumerate}
\end{lemma}

\vspace{3pt}
\begin{corollary}[First Borel-Cantelli Lemma] \label{BCL1}\ \\
Suppose $\sum_n \P(\abs{X_n - X} > \vep) < \infty,\ \forall\ \vep > 0$, then
\begin{equation*}
    X_n \asto X.
\end{equation*}
\end{corollary}
\begin{proof}\ \\
By Borel-Cantelli, $\sum_n \1_{\abs{X_n - X} > \vep} < \infty$ or $N_\vep < \infty$ a.s.. $\Rightarrow$ $\exists$ an a.s. set $\Omega_\vep$ s.t. 
\begin{equation*}
    N_\vep (\omega) < \infty,\ \forall\ \omega \in \Omega_\vep.
\end{equation*}
We need to show that there exists $\Omega_0$ s.t.
\begin{equation*}
    \forall\ \omega \in \Omega_0,\ N_\vep < \infty,\ \forall\ \vep > 0.
\end{equation*}
Observe that for $0 < \vep_1 < \vep_2$, we have 
\begin{equation*}
    N_{\vep_1} \geq N_{\vep_2}\ \Rightarrow\ \{N_{\vep_1} < \infty\} \subset \{N_{\ep_2} < \infty\}
\end{equation*}
Now choose $\vep_1 > \vep_2 > \dots,\ \lim_n \vep_n = 0$, and thus
\begin{equation*}
    \{N_{\vep_1} < \infty\} \supset \{N_{\vep_2} < \infty\} \supset \dots.
\end{equation*}
Define $\Omega_0 = \cap_{k} \{N_{\vep_k} < \infty\}\ \Rightarrow\ \P(\Omega_0) = 1$.

\np For $\omega \in \Omega,\ \vep > \vep_k$ $\Rightarrow$ $\Omega_0 \supset \{N_{\vep_k} < \infty\}$, and thus $N_\ep \leq N_{\ep_k} < \infty$. Complete the proof.
\end{proof}


\vspace{3pt}
\begin{corollary}[Second Borel Cantelli Lemma]\label{BCL2}\ \\
Suppose $(B_n)$ are independent Bernoulli sequence, if $\sum_n \E B_n = +\infty$, we have $\sum_n B_n = + \infty$ a.s..
\end{corollary}
\begin{proof}\ \\
Define $p_n = \E B_n,\ a_n = \sum_{i \leq n} p_i,\ S_n = \sum_{i \leq n} B_i,\ S = \lim_n S_n$. We have
\begin{equation*}
    \Var{S_n} = \summ{i=1}{n} p_i(1 - p_i) \leq \summ{i=1}{n} p_i = a_n
\end{equation*}
For $b \in (0,\infty)$, note $\sum_n \E B_n = +\infty\ \Rightarrow\ a_n \to \infty$, we have
\begin{equation*}
    a_n - \sqrt{b a_n} \nearrow +\infty,\ \{S < a_n - \sqrt{b a_n}\} \nearrow \{S < + \infty\}.
\end{equation*}
By $S_n \leq S$, we have
\begin{equation*}
    \{S < a_n - \sqrt{b a_n}\} \subset \{S_n < a_n - \sqrt{b a_n}\} \subset \{\abs{S_n - a_n} > \sqrt{b a_n}\}.
\end{equation*}
Therefore,
\begin{align*}
    \P(S < \infty) &= \lim_n \P(S < a_n - \sqrt{ba_n})\\
    &\leq \limsup_n \P(\abs{S_n - a_n} > \sqrt{b a_n})\\
    &\leq \limsup_n \frac{\Var{S_n}}{b a_n}\\
    &\leq \frac{1}{b},
\end{align*}
which implies $\P(S < \infty) = 0$, which implies $\sum_n B_n = +\infty$ a.s..
\end{proof}


\vspace{6pt}
\subsubsection{Convergence in Probability}    

\begin{definition}\ \\
$X_n \prto X$ if $\forall\ \vep  > 0,\ \lim_n \P(\abs{X_n - X} > \vep) = 0$, we say $X_n$ converges to $X$ in probability.
\end{definition}

\begin{example}\ \\
Typewriter function $X_n(x) = 1$ on $[\frac{k}{2^{n'}}, \frac{k + 1}{2^{n'}}]$, where $n' = \max{\{n' \in \N: 2^{n'} \leq n\}}, k = n - 2^{n'}$ and vanishes elsewhere. We have
\begin{equation*}
    \liminf_n X_n(\omega) = 0,\ \limsup_n X_n(\omega) = 1.
\end{equation*}
This is an example converging in probability but not almost surely.
\end{example}

\vspace{3pt}
\begin{lemma}\ \\
$X_n \asto X$ $\Rightarrow$ $X_n \prto X$.
\end{lemma}
\begin{proof}
DIY.
\end{proof}

\vspace{3pt}
\begin{lemma}\label{2.13}\ \\
If $X_n \prto X$, then $\exists$ a subsequence $(n_k)$ s.t. $X_{n_k} \asto X$.
\end{lemma}
\begin{proof}\ \\
Choose $\vep_k = \frac{1}{2^k}$, then find $n_k$ s.t.
\begin{equation*}
\P(\abs{X_{n_k} - X} > \vep_k) \leq \vep_k,    
\end{equation*}
which implies
\begin{equation*}
    \sum_k \P(\abs{X_{n_k} - X} > \vep_k) \leq \sum_n \vep_k = 1\ \Rightarrow\ X_{n_k} \asto X.
\end{equation*}
\end{proof}

\vspace{3pt}
\begin{theorem}[$\Ls_0$ with convergence in probability is metrizable]\ \\
Define a metric $d(X,Y):= \E[\abs{X - Y} \wedge 1] = \E f(\abs{X-Y}),\ f(x) = x \wedge 1,\ x \geq 0$. We have
\begin{itemize}
    \item $d(X,Y) = 0\ \Leftrightarrow\ X = Y \text{ a.s.}$.
    \item $d(X,Y) + d(Y,Z) \geq d(X,Z)$ ($f(a) + f(b) \geq f(a+b)$).
\end{itemize}
Furthermore, if $X_n \prto X$, we have $d(X_n, X) \to 0$.
\end{theorem}
\begin{proof}\ 
\begin{equation*}
\begin{aligned}
        f(\vep) \1_{(\vep,\infty)} (X) &\leq f(X) \leq f(\vep) + \1_{(\vep,\infty)}(X)\\
        f(\vep) \1_{(\vep,\infty)} (\abs{X_n - X}) &\leq f (\abs{X_n - X}) \leq f(\vep) + \1_{(\vep,\infty)} (\abs{X_n - X}),
\end{aligned}
\end{equation*}
which implies
\begin{equation*}
    f(\vep)\P(\abs{X_n - X} > \vep) \leq d(X_n ,X) \leq f(\vep) + \P(\abs{X_n - X} > \vep) \to 0.
\end{equation*}
\end{proof}

\vspace{3pt}
\begin{theorem}\ \\
$X_n \prto X\ \Leftrightarrow$ every subsequence of $(X_n)$ has a further subsequence that converges to X a.s..
\end{theorem}
\begin{proof}\ 
\begin{itemize}
    \item $\Rightarrow:$\\
    $X_n \prto X$ $\Rightarrow$ every subsequence converges to $X$ in probability, then by \hyperref[2.13]{Lemma 2.13}, $\exists$ a further subsequence that converges to $X$ a.s..
    \item $\Leftarrow:$\\
    Suppose $X_n \not\prto X$, need to show $\exists\ I \subseteq N$ s.t. for every subsequence $J \subseteq I$, we have $(X_n)_{n \in J}$ does not converge to $X$ a.s.. We have
    \begin{equation*}
        P_n(\vep) = \P(\abs{X_n - X} > \vep) \not \to 0.
    \end{equation*}
    Define $\delta = \limsup_n P_n(\vep) > 0$. There is $I \subseteq N$ s.t. $P_n(\vep) \to \delta > 0$. Let $J \subseteq I$, if there exists $(X_n)_{n \in J} \asto X$, then $(X_n)_{n \in J} \prto X$, which implies $\lim_{n \in J} P_n(\vep) = 0$ which can't be. 
\end{itemize}
\end{proof}

\vspace{3pt}
\begin{corollary}\ \\
$X_n \prto X,\ Y_n \prto Y$ $\Rightarrow$ $X_n + Y_n \prto X + Y$.
\end{corollary}
\begin{proof}\ \\
Pick a subsequence $I \subseteq \N$, we have $X_n \prto X$, which means $\exists\ J \subset I$ s.t.
\begin{equation*}
    (X_n)_{n \in J} \asto X.
\end{equation*}
For $Y_n \prto Y$ $\Rightarrow$ $\exists\ K \subset I$ s.t.
\begin{equation*}
    (Y_n)_{n \in (I \cap J)} \asto Y,\ (X_n)_{n \in K} \asto X,
\end{equation*}
and thus $(X_n + Y_n)_{n \in K} \asto (X + Y)$.
\end{proof}


\begin{corollary}\ \\
$X_n \prto X$ $\Rightarrow$ $f(X_n) \prto f(X)$ for continuous $f$.
\end{corollary}








