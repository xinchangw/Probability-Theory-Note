\subsection{Essential Supremum of a family of Random Variables}

\np For measure space $(\Omega, \Fs, \P)$, $\Phi$ an arbitrary famaly of random variables.\\
First case $\Phi$ countable,
    \begin{center}
        $\varphi^*(\omega) = \sup_{\varphi \in \Phi} \varphi(\omega)$ will also be a random variable.
    \end{center}



\begin{example}\ \\
$\Phi = \{\1_{x}:\ 0 < x \leq 1\}$, and $\Omega = [0,1],\ \P = \text{ Lebesgue measure}$, then $\sup_{\varphi \in \Phi} \varphi(\omega) = 1$.
\end{example}

\begin{theorem}\ \\
Let $\Phi$ be an arbitrary collection of eandom variables on $(\Omega, \Fs, \P)$,
\begin{enumerate}[label = (\alph*)]
    \item $\exists$ a random variable $\vphi^*$ s.t. $\vphi^* \geq \vphi$ $\P$-a.s. for all $\vphi \in \Phi$. Moreover, $\vphi^*$ is $\P$-a.s. unique:
    \begin{center}
         any other random variable $\vphi$ satisfies $\vphi \geq \vphi^*$ $\P$-a.s.
    \end{center}
    \item Suppose $\Phi$ is directed upwards (i.e. $\vphi \& \Tilde{\vphi} \in \Phi$, $\exists\ \psi \in \Phi$ s.t. $\psi \geq \vphi \vee \title{\vphi}$), which implies
    \begin{center}
        $\exists$ an increasing sequence $(\vphi_n)$ of r.v's in $\Phi$ s.t. $\vphi^* = \lim_{n} \vphi_n$ $\P$-a.s.
    \end{center}
\end{enumerate}
\end{theorem}
\begin{proof}\ 
\begin{enumerate}[label = (\arabic*)]
    \item WLOG, $\vphi \in \Phi$ takes values in $[0,1]$, otherwise 
\begin{equation*}
    \Tilde{\Phi} = \{f \circ \vphi:\ \vphi \in \Phi\}
\end{equation*}
    with $f : \R \to [0,1]$ is strictly increasing.
    \item If $\Psi (\subset \Phi)$ is countable, then $\vphi_{\Psi} (\omega) = \sup_{\vphi \in \Psi} \vphi(\omega)$ is measurable.\\
We claim that $c:= \sup \{\E[\vphi_{\Psi}]\ |\ \Psi \subset \Phi \text{ countable } \}$ is attained by countable $\Psi^* \subseteq \Phi$.\\
Let $(\Psi_n)$ be a maximizing sequence (i.e. $\E[\vphi_{\Psi_n}] \uparrow c$), we have
\begin{equation*}
    \Psi^* = \cup_n \Psi_n.
\end{equation*}
is also countable, and by MCT
\begin{equation*}
    \E[\underbrace{\vphi_{\Psi^*}}_{\vphi^*}] = c.
\end{equation*}
    \item We have $\vphi^* \geq \vphi \text{ a.s.},\ \vphi \in \Phi$. If not, $\exists$ a r.v. $\vphi \in \Phi$ s.t.
    \begin{equation*}
        \P(\vphi \geq \vphi^*) > 0.
    \end{equation*}
    Define $\Psi' = \Psi^* \cup \{\vphi\}$, we have
    \begin{equation*}
        \E[\vphi_{\Psi'}] > c,
    \end{equation*}
    which gives a contradiction.
    \item The uniqueness is easy.
\end{enumerate}
\end{proof}

\np Simple observation:\\
$H_n \downarrow H \Rightarrow \P(H_n) \downarrow \P(H)$, and if $H_1 \supset H_2 \supset \dots$, we have $\lim_n H_n = \cap_n H_n = H$. Then 
\begin{equation*}
    H_n^c \uparrow H^c \Rightarrow \P(H_n^c) \to \P(H^c),\ 1 - \P(H_n) \uparrow 1 - \P(H) 
\end{equation*}

\np We say $\mu$ is a distribution of $X$ by defining $\mu(A) = \P(X^{-1} A)$, and we have c.d.f $c(x) = \mu(-\infty,x]$, which is right continuous, increasing and $c(\infty) = 1$.

\vspace{12pt}
\subsection{Properties of Random Variables}
\begin{definition}[Independence]\ \\
For a probability space $(\Omega, \Hs,\P)$, we say $\Fs \indep \Gs$ provided $\P(A \cap B) = \P(A) \P(B),\ A \in \Fs,\ B \in \Gs$, where $\Fs,\Gs$ are two subalgebras of $\Hs$.
\end{definition}

\begin{remark}\ \\
Suppose $X,Y$ are two r.v.s $(\sigma(X), \sigma(Y))$, we have $ X \indep Y$ provided $\sigma(X)$ and $\sigma(Y)$ are independent. $X \indep \Gs$ if $\sigma(X)\ \&\ \Gs$ are independent.\\
Equivalently, $X \indep Y$ if $\pi(A \times B) = \P(X \in A,\ Y \in B) = \P(X \in A) \P(Y \in B)$.
\end{remark}

\np Recall that $\E[f_0 X] = \mu f$,
\begin{example}\ \\
$\mu(dx) = \frac{c^a x^{a - 1} e^{-cx}}{\Gamma(a)} \dr x,\ x \in \R^+$, where $a$ is the index parameter, $c$ is the scale parameter, and $\Gamma$ be the Gamma function. We have
\begin{equation*}
    \Gamma(a) = \int_0^\infty x^{a-1} e^{-x} \dr x,\ \Gamma(n) = (n-1)!,\ \Gamma(\frac{1}{2}) = \sqrt{2\pi}.
\end{equation*}
Let $X\ \&\ Y$ have Gamma distribution and $X \indep Y$, i.e. $X \sim \gamma_a,\ Y \sim \gamma_b$ with scale parameter $1$, we have
\begin{enumerate}[label = (\alph*)]
    \item $X + Y \sim \gamma_{a + b}$.
    \item $\frac{X}{X+Y}$ has distribution $\beta_{a,b} =\dr u\ \frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)}u^{a - 1} (1 - u)^{b-1}$.
    \item $X + Y \indep \frac{X}{X+Y}$.
    \begin{proof}
    The joint distribution $\pi$ on $(X+Y, \frac{X}{X+Y})$ is defined
    \begin{align*}
        \pi f &= \E f(X+Y, \frac{X}{X+Y})\\ 
        &= \int_0^\infty \dr x\ \frac{x^{a - 1} e^{-x}}{\Gamma(a)} \int_0^\infty \dr y\ \frac{e^{-y} x^{b - 1}}{\Gamma(b)} f(x + y, \frac{x}{x + y})\\
        &= \int_0^\infty \dr z\ \int_0^1 \dr u\ \frac{z^{a+b-1}e^{-z}}{\Gamma(a) \Gamma(b)} u^{a-z} (1 - u)^{b-1} f(z,u)  &(x \to uz,\ y = (1 - u)z)\\
        &= (\gamma_{a+b} \times \beta_{a+b})f
    \end{align*}
    \end{proof}
\end{enumerate}
\end{example}

\begin{example}\ \\
Suppose $X \sim \Ns(0,1)$, then we have the distribution $\nu$ of $X$ is given by
\begin{equation*}
    \nu(\dr x) = \dr x\ \frac{e^{x^2/2}}{\sqrt{2\pi}}.
\end{equation*}
Let $\mu$ be the distribution of $X^2$, then
\begin{equation*}
    \mu f = \E f \circ X^2 = \E g \circ X = \nu g,\ g(x) = f(x^2),
\end{equation*},
we further have 
\begin{equation*}
    \mu f = \int \dr x\ e^{-x^2/2} g(x) = \int \dr x\ e^{-x^2/2} f(x^2) = \int\ \mu( \dr y) f(y),
\end{equation*}
and thus
\begin{equation*}
    \mu(\dr y) = \dr y\ \frac{1}{\sqrt{2\pi}} \frac{e^{-y}}{\sqrt{y}} = \frac{\dr y\ e^{-y} y^{-1/2}}{\Gamma(1/2)}
\end{equation*}
\end{example}

\begin{example}\ \\
For $X \geq 0$, $r \to \E e^{-r X}$ decreasing continuous. In fact $\exists\ R$ r.v. s.t. 
\begin{equation*}
    \underbrace{\P(R > r)}_{\text{Survival Function}} = \E e^{-r X}.
\end{equation*}
Take $R = \frac{Y}{X},\ Y \indep X,\ Y \sim \exp$, which means $\P(Y > x) = e^{-x}$. We have
\begin{align*}
    \P(R > r) &= \P(Y > rX)\\
    &= \int_{\R_+} \mu(\dr x) \int_{\R_+}\dr y\ e^{-y} \1_{(rx,+\infty)}(y)\\
    &= \int \mu(\dr x) e^{-rx}\\
    &= \E e^{-r X}.
\end{align*}
\end{example}

\begin{example}\ \\
For $X \sim \gamma_{a,c}$, $\P(R > r) = \E e^{-rx} = (\frac{c}{c + r})^a$. ($R = \frac{X}{Y}$)
\end{example}

 
\begin{definition}\ 
\begin{itemize}
    \item $\E X^n$ is called the \textit{$n$th moment} of $X$.
    \item $\E[(X - \E X)^2]$ is called the \textit{variance} of $X$.
    \item $\hat{\mu}_r = \E e^{-rX}$ the \textit{Laplace transform} of $X$, useful for $X \geq 0$.
    \item $\widetilde{\mu}_r = \E e^{irX}$ the \textit{Fourier transform} of $X$ (also \textit{characteristic function}).
    \item $\E z^X = \sum_{n = 0}^\infty z^n \P(X = n)$ is called the \textit{moment generating function}.
    \item For $X \geq 0$, $\E X^p = \int_0^\infty \dr x\ p x^{p-1} \P(X > x)$, where
    \begin{equation*}
        \E X^p(\omega) = \int_0^{X(\omega)} \dr\ x\ p x^{p-1} \E[\1_{X > x}]
    \end{equation*}
    by Fubini's theorem.
\end{itemize}
\end{definition}

\begin{proposition}[Markov's Inequality]\ \\
For $X \geq 0,\ b > 0$, we have
\begin{equation*}
    \P(X > b) \leq \frac{\E X}{b}.
\end{equation*}
\end{proposition}
\begin{proof}\ \\
We have $X \geq b \1_{X > b}$, and integrate on both side, we get
\begin{equation*}
    \E X \geq b \E \1_{X > b},
\end{equation*}
which gives the inequality for free.
\end{proof}

\begin{proposition}[Chebyshev Inequality]\ \\
Suppose $\E X = a < \infty$, we have
\begin{equation*}
    \P[(X - a)^2 > \vep^2] \leq \frac{1}{\vep^2} \E[(X-a)^2],
\end{equation*}
or equivalently,
\begin{equation*}
    \P[\abs{X - a} > \vep] \leq \frac{\Var{X}}{\vep^2}
\end{equation*}
\end{proposition}


\vspace{6pt}
\begin{theorem}[Weierstrass Approximation]\ \\
If $f$ is continuous on $[0,1]$ and $\vep > 0$ then $\exists$ a polynomial $B$ s.t.
\begin{equation*}
    \sup_{x \in [0,1]} \abs{B(x) - f(x)} \leq \vep.
\end{equation*}
\end{theorem}
\begin{proof}
We have $\P(S_n = k) = \binom{n}{k}p (1 - p)^{n - k}$ the Binomial distribution. Define
\begin{equation*}
    B_n(p) = \E[f(\frac{S_n}{n})] = \sum_{k = 0}^n f(\frac{k}{n}) \underbrace{\binom{n}{k} p^k (1 - p)^{n - k}}_{\text{Bernstein Polynomials}}.
\end{equation*}
Note that $\exists\ K > 0$ s.t. $\abs{f(y)} \leq K$ and f is uniformly continuous ($\abs{x - y} \leq \delta \Rightarrow \abs{f(x) - f(y)} \leq \frac{1}{2} \ep$). We have
\begin{align*}
    \abs{B_n(p) - f(p)} = \abs{\E[f(\frac{S_n}{n}) - f(p)]}.
\end{align*}
Define $Y_n = \abs{f(\frac{S_n}{n}) - f(p)},\ Z_n = \abs{\frac{S_n}{n} - p}$, and
\begin{equation*}
    Z_n \leq \delta \Rightarrow Y_n < \frac{1}{2}.
\end{equation*}
Therefore,
\begin{align*}
    \abs{B_n(p) - f(p)} &\leq \E Y_n\\
    &= \E[Y_n \1_{Z_n \leq \delta}] + \E[Y_n \1_{Z_n > \delta}]\\
    &= \frac{1}{2} \ep + K \P(Z_n > \delta).
\end{align*}
Note by Chebyshev Inequality have
\begin{equation*}
    \P(Z_n > \delta) \leq \frac{\Var{\frac{S_n}{n}}}{\delta^2} = \frac{\frac{1}{n^2} \Var{S_n}}{\delta^2} = \frac{p(1 - p)}{n \delta^2} \leq \frac{1}{4n\delta^2}.
\end{equation*}
Thus we have $ \sup_{x \in [0,1]} \abs{B(x) - f(x)} \to 0$ as $n \to \infty$.
\end{proof}











